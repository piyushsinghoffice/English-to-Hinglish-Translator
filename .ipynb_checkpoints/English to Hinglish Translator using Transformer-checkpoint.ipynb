{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068d5257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f9d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Hinglish-TOP-Dataset-main/Dataset/Human-Annotated-Data/train.tsv\", sep='\\t')\n",
    "test_df = pd.read_csv(\"Hinglish-TOP-Dataset-main/Dataset/Human-Annotated-Data/test.tsv\", sep='\\t')\n",
    "val_df = pd.read_csv(\"Hinglish-TOP-Dataset-main/Dataset/Human-Annotated-Data/validation.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e920816e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_query</th>\n",
       "      <th>cs_query</th>\n",
       "      <th>en_parse</th>\n",
       "      <th>cs_parse</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Add a new weekly reminder for Sunday Brunch at...</td>\n",
       "      <td>9 : 30 am ko Sunday Brunch ke liye ek naya wee...</td>\n",
       "      <td>[IN:CREATE_ALARM Add a new [SL:PERIOD weekly ]...</td>\n",
       "      <td>[IN:CREATE_ALARM [SL:DATE_TIME 9 : 30 am ko ] ...</td>\n",
       "      <td>alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>message danny and see if he wants to go to com...</td>\n",
       "      <td>danny ko message karo aur dekho ke he wants to...</td>\n",
       "      <td>[IN:SEND_MESSAGE message [SL:RECIPIENT danny ]...</td>\n",
       "      <td>[IN:SEND_MESSAGE [SL:RECIPIENT danny ] ko mess...</td>\n",
       "      <td>messaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>set alarm for 2 hours</td>\n",
       "      <td>do ghante ke liye alarm set kardo</td>\n",
       "      <td>[IN:CREATE_ALARM set alarm [SL:DATE_TIME for 2...</td>\n",
       "      <td>[IN:CREATE_ALARM [SL:DATE_TIME do ghante ke li...</td>\n",
       "      <td>alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kill the reminder for baking a cake for neil</td>\n",
       "      <td>neil ke liye cake bake karne ke reminder ko mi...</td>\n",
       "      <td>[IN:DELETE_REMINDER kill the reminder for [SL:...</td>\n",
       "      <td>[IN:DELETE_REMINDER [SL:TODO neil ke liye cake...</td>\n",
       "      <td>reminder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retrieve my chat requests please</td>\n",
       "      <td>Please mere chat requests ko retrieve kare</td>\n",
       "      <td>[IN:GET_MESSAGE retrieve my chat requests plea...</td>\n",
       "      <td>[IN:GET_MESSAGE Please mere chat requests ko r...</td>\n",
       "      <td>messaging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            en_query  \\\n",
       "0  Add a new weekly reminder for Sunday Brunch at...   \n",
       "1  message danny and see if he wants to go to com...   \n",
       "2                              set alarm for 2 hours   \n",
       "3       kill the reminder for baking a cake for neil   \n",
       "4                   retrieve my chat requests please   \n",
       "\n",
       "                                            cs_query  \\\n",
       "0  9 : 30 am ko Sunday Brunch ke liye ek naya wee...   \n",
       "1  danny ko message karo aur dekho ke he wants to...   \n",
       "2                  do ghante ke liye alarm set kardo   \n",
       "3  neil ke liye cake bake karne ke reminder ko mi...   \n",
       "4         Please mere chat requests ko retrieve kare   \n",
       "\n",
       "                                            en_parse  \\\n",
       "0  [IN:CREATE_ALARM Add a new [SL:PERIOD weekly ]...   \n",
       "1  [IN:SEND_MESSAGE message [SL:RECIPIENT danny ]...   \n",
       "2  [IN:CREATE_ALARM set alarm [SL:DATE_TIME for 2...   \n",
       "3  [IN:DELETE_REMINDER kill the reminder for [SL:...   \n",
       "4  [IN:GET_MESSAGE retrieve my chat requests plea...   \n",
       "\n",
       "                                            cs_parse     domain  \n",
       "0  [IN:CREATE_ALARM [SL:DATE_TIME 9 : 30 am ko ] ...      alarm  \n",
       "1  [IN:SEND_MESSAGE [SL:RECIPIENT danny ] ko mess...  messaging  \n",
       "2  [IN:CREATE_ALARM [SL:DATE_TIME do ghante ke li...      alarm  \n",
       "3  [IN:DELETE_REMINDER [SL:TODO neil ke liye cake...   reminder  \n",
       "4  [IN:GET_MESSAGE Please mere chat requests ko r...  messaging  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32bca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Pairs\n",
    "train_pairs = []\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    eng = row['en_query']\n",
    "    hnd = row['cs_query']\n",
    "    hnd = \"[start] \" + hnd + \" [end]\"\n",
    "    train_pairs.append((eng, hnd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c73fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Pairs\n",
    "test_pairs = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    eng = row['en_query']\n",
    "    hnd = row['cs_query']\n",
    "    hnd = \"[start] \" + hnd + \" [end]\"\n",
    "    test_pairs.append((eng, hnd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dbbcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation Pairs\n",
    "val_pairs = []\n",
    "\n",
    "for _, row in val_df.iterrows():\n",
    "    eng = row['en_query']\n",
    "    hnd = row['cs_query']\n",
    "    hnd = \"[start] \" + hnd + \" [end]\"\n",
    "    val_pairs.append((eng, hnd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc3ded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('parties in san francisco during the month of december', '[start] san francisco me december ke mahine ke dauran parties [end]')\n",
      "('What churches are having Christmas Eve services near me', '[start] mere paas kaunse churches mei Christmas Eve services hai [end]')\n",
      "('What is the high temperature for today', '[start] aaj ke liye zyada temperature kya hai [end]')\n",
      "('Delete my timer', '[start] Mere timer ko delete kare [end]')\n",
      "('Will it drop below freezing tonight', '[start] Kya ye aaj raat freezing se niche gir jayega [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b30b3337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2993 training pairs\n",
      "1390 validation pairs\n",
      "6513 test pairs\n"
     ]
    }
   ],
   "source": [
    "# random.shuffle(text_pairs)\n",
    "# num_val_samples = int(0.15 * len(text_pairs))\n",
    "# num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "# train_pairs = text_pairs[:num_train_samples]\n",
    "# val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "# test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "# print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e734e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextVector():\n",
    "    def __init__(self, max_tokens, encoder='word', output_sequence_length=None, add_sequence_length=0, padding='post', standardization=None):\n",
    "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "        from tokenizers import CharBPETokenizer\n",
    "        self.sequence_length = output_sequence_length\n",
    "        self.add_sequence_length = add_sequence_length\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        \n",
    "        if encoder == 'word':\n",
    "            self.tokenizer = Tokenizer(num_words=max_tokens, oov_token='<OOV>', filters='!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n')\n",
    "        elif encoder == 'CharBPE':\n",
    "            self.tokenizer = CharBPETokenizer()\n",
    "\n",
    "        self.padding = padding\n",
    "        self.pad_sequences = pad_sequences\n",
    "        self.standardization = standardization\n",
    "  \n",
    "    def adapt(self, data):\n",
    "        if self.standardization != None:\n",
    "            data = map(self.standardization, data)\n",
    "            \n",
    "        if self.encoder == 'word':\n",
    "            self.tokenizer.fit_on_texts(data)\n",
    "            self.word_index = self.tokenizer.word_index\n",
    "            \n",
    "        elif self.encoder == 'CharBPE':\n",
    "            self.tokenizer.train_from_iterator(data)\n",
    "            self.word_index = self.tokenizer.get_vocab()\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        if self.encoder == 'word':\n",
    "            self.encoded = self.tokenizer.texts_to_sequences(sentence)\n",
    "            result = self.pad_sequences(encoded, maxlen = self.sequence_length, padding=self.padding, value=0.0)\n",
    "            \n",
    "        elif self.encoder == 'CharBPE':\n",
    "            self.encoded_batch = self.tokenizer.encode_batch(sentence)\n",
    "            result = [encoded.ids for encoded in self.encoded_batch]\n",
    "            result = self.pad_sequences(result, maxlen = self.sequence_length, padding=self.padding)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f109c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 5000\n",
    "sequence_length = 56\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(str(input_string))\n",
    "    result = tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "    result = result.numpy().decode()\n",
    "    return result\n",
    "\n",
    "\n",
    "# eng_vectorization = CustomTextVector(\n",
    "#     max_tokens=vocab_size, encoder='CharBPE', output_sequence_length=sequence_length\n",
    "# )\n",
    "hin_vectorization = CustomTextVector(\n",
    "    max_tokens=vocab_size,\n",
    "    encoder='CharBPE', output_sequence_length=sequence_length + 1,\n",
    "    standardization=custom_standardization\n",
    ")\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_hin_texts = [pair[1] for pair in train_pairs]\n",
    "hin_vectorization.adapt([train_eng_texts, train_hin_texts])\n",
    "# hin_vectorization.adapt(train_hin_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66eb47fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hin_vectorization.tokenizer.add_tokens(['[start]', '[end]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d4f6c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4083,  376,  114,  521,  252,  352,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hin_vectorization(['hello kese ho are you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e04bee5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello kese ho are you'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hin_vectorization.tokenizer.decode([4083,  376,  114,  521,  252,  352,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "debcba91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [5, 6, 7]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4], [5,6,7,8]])\n",
    "a[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfd3b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, hin):\n",
    "    return ({\"encoder_inputs\": eng[:,:-1], \"decoder_inputs\": hin[:, :-1],}, hin[:, 1:])\n",
    "\n",
    "def sentence_to_tokens(pair):\n",
    "    eng = hin_vectorization(pair[0])\n",
    "    hin = hin_vectorization(pair[1])\n",
    "    return (eng, hin)\n",
    "   \n",
    "def make_dataset(pairs):\n",
    "    eng_texts, hin_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    hin_texts = list(hin_texts)\n",
    "    data = (eng_texts, hin_texts)\n",
    "    data = sentence_to_tokens(data)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data[0], data[1]))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5fffccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 56)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 56)\n",
      "targets.shape: (64, 56)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a82f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"decoder_inputs\"]: [[4711 2228  105 ...    0    0    0]\n",
      " [4711  242  622 ...    0    0    0]\n",
      " [4711  476  569 ...    0    0    0]\n",
      " ...\n",
      " [4711  284 2951 ...    0    0    0]\n",
      " [4711 1183  343 ...    0    0    0]\n",
      " [4711  242   54 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"decoder_inputs\"]: {inputs[\"decoder_inputs\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33584ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        \n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f87c8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 4096\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb942fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4713"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hin_vectorization.tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3995e9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4713"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hin_vectorization.tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5f136cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   1294336     ['encoder_inputs[0][0]']         \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   4206080     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 5000)   8889480     ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,389,896\n",
      "Trainable params: 14,389,896\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "47/47 [==============================] - 16s 182ms/step - loss: 1.1684 - accuracy: 0.1759 - val_loss: 1.0173 - val_accuracy: 0.2778\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 7s 157ms/step - loss: 0.9083 - accuracy: 0.3183 - val_loss: 0.8787 - val_accuracy: 0.3492\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 7s 157ms/step - loss: 0.7800 - accuracy: 0.3800 - val_loss: 0.8267 - val_accuracy: 0.3742\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 7s 160ms/step - loss: 0.7024 - accuracy: 0.4165 - val_loss: 0.7761 - val_accuracy: 0.4093\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 8s 162ms/step - loss: 0.6400 - accuracy: 0.4551 - val_loss: 0.7479 - val_accuracy: 0.4238\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 8s 163ms/step - loss: 0.5901 - accuracy: 0.4812 - val_loss: 0.7206 - val_accuracy: 0.4416\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 8s 164ms/step - loss: 0.5398 - accuracy: 0.5128 - val_loss: 0.7253 - val_accuracy: 0.4458\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 8s 164ms/step - loss: 0.5024 - accuracy: 0.5409 - val_loss: 0.7059 - val_accuracy: 0.4473\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 8s 171ms/step - loss: 0.4615 - accuracy: 0.5687 - val_loss: 0.7045 - val_accuracy: 0.4624\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 0.4223 - accuracy: 0.5997 - val_loss: 0.6969 - val_accuracy: 0.4562\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 0.3899 - accuracy: 0.6252 - val_loss: 0.6883 - val_accuracy: 0.4717\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.3511 - accuracy: 0.6555 - val_loss: 0.6885 - val_accuracy: 0.4761\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 8s 175ms/step - loss: 0.3195 - accuracy: 0.6820 - val_loss: 0.7117 - val_accuracy: 0.4750\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 8s 179ms/step - loss: 0.2941 - accuracy: 0.7030 - val_loss: 0.7029 - val_accuracy: 0.4813\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 0.2605 - accuracy: 0.7355 - val_loss: 0.7048 - val_accuracy: 0.4855\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 0.2380 - accuracy: 0.7545 - val_loss: 0.7155 - val_accuracy: 0.4755\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 8s 176ms/step - loss: 0.2082 - accuracy: 0.7866 - val_loss: 0.7189 - val_accuracy: 0.4843\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 8s 180ms/step - loss: 0.1884 - accuracy: 0.8056 - val_loss: 0.7303 - val_accuracy: 0.4903\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 8s 178ms/step - loss: 0.1668 - accuracy: 0.8266 - val_loss: 0.7386 - val_accuracy: 0.4791\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 8s 176ms/step - loss: 0.1489 - accuracy: 0.8468 - val_loss: 0.7454 - val_accuracy: 0.4912\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.1344 - accuracy: 0.8624 - val_loss: 0.7454 - val_accuracy: 0.4902\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.1212 - accuracy: 0.8746 - val_loss: 0.7513 - val_accuracy: 0.4885\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.1100 - accuracy: 0.8881 - val_loss: 0.7645 - val_accuracy: 0.4905\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.0995 - accuracy: 0.9000 - val_loss: 0.7638 - val_accuracy: 0.4898\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 0.0931 - accuracy: 0.9041 - val_loss: 0.7710 - val_accuracy: 0.4918\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 0.0857 - accuracy: 0.9124 - val_loss: 0.7882 - val_accuracy: 0.4922\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 0.0765 - accuracy: 0.9223 - val_loss: 0.7987 - val_accuracy: 0.4835\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 0.0753 - accuracy: 0.9245 - val_loss: 0.8015 - val_accuracy: 0.4897\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 8s 177ms/step - loss: 0.0688 - accuracy: 0.9307 - val_loss: 0.7954 - val_accuracy: 0.4947\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 8s 177ms/step - loss: 0.0670 - accuracy: 0.9330 - val_loss: 0.8059 - val_accuracy: 0.4910\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 0.0622 - accuracy: 0.9376 - val_loss: 0.8133 - val_accuracy: 0.4941\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 0.0628 - accuracy: 0.9375 - val_loss: 0.8048 - val_accuracy: 0.4951\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 0.0567 - accuracy: 0.9437 - val_loss: 0.8127 - val_accuracy: 0.4916\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.0554 - accuracy: 0.9448 - val_loss: 0.8079 - val_accuracy: 0.4953\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.0539 - accuracy: 0.9481 - val_loss: 0.8156 - val_accuracy: 0.4969\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.0562 - accuracy: 0.9492 - val_loss: 0.8141 - val_accuracy: 0.4981\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.0492 - accuracy: 0.9553 - val_loss: 0.8266 - val_accuracy: 0.4932\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.0528 - accuracy: 0.9514 - val_loss: 0.8014 - val_accuracy: 0.4979\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.0472 - accuracy: 0.9589 - val_loss: 0.8139 - val_accuracy: 0.5012\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.0474 - accuracy: 0.9576 - val_loss: 0.8174 - val_accuracy: 0.5014\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 0.0476 - accuracy: 0.9580 - val_loss: 0.8133 - val_accuracy: 0.5022\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.0441 - accuracy: 0.9626 - val_loss: 0.8246 - val_accuracy: 0.4977\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 0.0438 - accuracy: 0.9640 - val_loss: 0.8067 - val_accuracy: 0.5023\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 0.0453 - accuracy: 0.9618 - val_loss: 0.8152 - val_accuracy: 0.4961\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 0.0436 - accuracy: 0.9634 - val_loss: 0.8250 - val_accuracy: 0.4867\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 0.0429 - accuracy: 0.9639 - val_loss: 0.8308 - val_accuracy: 0.4956\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 0.0410 - accuracy: 0.9670 - val_loss: 0.8235 - val_accuracy: 0.4970\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 0.0406 - accuracy: 0.9690 - val_loss: 0.8189 - val_accuracy: 0.4998\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 0.0420 - accuracy: 0.9653 - val_loss: 0.8200 - val_accuracy: 0.4987\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 0.0407 - accuracy: 0.9674 - val_loss: 0.8151 - val_accuracy: 0.5029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241d4f41fd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50  # This should be at least 30 for convergence\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb3b467a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about the annual Tangle Town garage sale that 's coming up soon\n",
      "[start] u mein weather ke baare me hur rist hai day s jo night house u ko ke 2019 like baare d [end]\n",
      "                                        \n",
      "Play all Kenny Chesney\n",
      "[start] l ko play karo ki go to sleep [end]\n",
      "                                        \n",
      "abolish all alarms that I have made\n",
      "[start] mere saare set kiye huwe alarms ko nikaal do [end]\n",
      "                                        \n",
      "Play Creed ' s new album\n",
      "[start] ek naya album bajao [end]\n",
      "                                        \n",
      "What ' s the traffic report for Portland\n",
      "[start] or may ke liye mausam kaisa hai [end]\n",
      "                                        \n",
      "remind me to cancel all monthly automatic payments that i have set up\n",
      "[start] mujhe yaad dilaayen ki is set mujhe ek ghante mei kitna time mujhe mere paas pehle [end]\n",
      "                                        \n",
      "What is the traffic like from Pensacola to Gulf Breeze\n",
      "[start] the family group a traffic kaisa hai [end]\n",
      "                                        \n",
      "remind me to mail my water bill today at 6 pm .\n",
      "[start] is evening mere area mei kya free events ke liye yaad dilaayen [end]\n",
      "                                        \n",
      "What ' s happening today\n",
      "[start] aj kya ho raha he [end]\n",
      "                                        \n",
      "Alert me at 4 pm tomorrow\n",
      "[start] kal 4 pm mujhe aaj raat ka ek alarm set karen [end]\n",
      "                                        \n",
      "remind me to pick up the mail\n",
      "[start] mujhe yaad dilao ki mujhe to make [end]\n",
      "                                        \n",
      "Wake me up at 7 am\n",
      "[start] 7 am ke liye alarm banaye [end]\n",
      "                                        \n",
      "Remind me to make dinner tonight around 4 this afternoon .\n",
      "[start] u aj raat meri playlist ko mujhe kitna samay lagega [end]\n",
      "                                        \n",
      "How many miles is Dallas from Houston\n",
      "[start] a al kitne alarms set kiye hue he [end]\n",
      "                                        \n",
      "Update the reminder to taking out the trash\n",
      "[start] party reminder set kijiye to take out for the ko hata do [end]\n",
      "                                        \n",
      "What is the traffic from Virginia Beach to Williamsburg\n",
      "[start] on se high kya hai [end]\n",
      "                                        \n",
      "what are my reminders for next week\n",
      "[start] agle hafte ke liye mere reminders kya hai [end]\n",
      "                                        \n",
      "are there any beer festivals this month\n",
      "[start] kya is mahine koi festival hai [end]\n",
      "                                        \n",
      "Remind me of my dinner reservations tonight\n",
      "[start] u saarey alarms bataiye ki doctor ki appointment schedule karne ke liye yaad dilaye [end]\n",
      "                                        \n",
      "When should I leave to make it on time\n",
      "[start] mujhe subah 9 bajhe tak office ke liye yaad dilaye [end]\n",
      "                                        \n",
      "Send a message back to my cousin George\n",
      "[start] mere message bhejo ki birthday party ke liye ek video message bhejo [end]\n",
      "                                        \n",
      "What ' s the weather forecast for the rest of the week\n",
      "[start] baaki hafte ke liye weather forecast kya hai [end]\n",
      "                                        \n",
      "play me some hip hop music please\n",
      "[start] please mere liye kuch 80s ka music play kare [end]\n",
      "                                        \n",
      "cancel the reminder with my brother tomorrow afternoon\n",
      "[start] mere daily reminder ko cancel karo to cancel kardo [end]\n",
      "                                        \n",
      "What ' s there to do this weekend in Orlando\n",
      "[start] is weekend kya us street par kya hone wala hai [end]\n",
      "                                        \n",
      "I need to know my reminders for work today\n",
      "[start] u u u ke liye mere reminder ki zarurat ki zarurat hai [end]\n",
      "                                        \n",
      "When will it be sunny again ?\n",
      "[start] o alarms kab hoga [end]\n",
      "                                        \n",
      "what ' s the forecast for the Oregon Coast this weekend\n",
      "[start] o o me kya forecast kya hai [end]\n",
      "                                        \n",
      "Play last song again .\n",
      "[start] last song ko phir se bajao [end]\n",
      "                                        \n",
      "Set a timer for 20 minutes .\n",
      "[start] 20 minutes ke liye timer set kare [end]\n",
      "                                        \n",
      "[start] 20 minutes ke liye timer set kare [end]\n"
     ]
    }
   ],
   "source": [
    "hin_vocab = hin_vectorization.word_index\n",
    "hin_index_lookup = dict(zip(range(1,len(hin_vocab)), hin_vocab))\n",
    "\n",
    "max_decoded_sentence_length = 56\n",
    "\n",
    "hin_index_lookup[0] = \"\"\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = np.array(hin_vectorization([input_sentence]))\n",
    "#     print(\"tokenized_input_sentence: \", tokenized_input_sentence[0])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = hin_vectorization([decoded_sentence])[:, :-1]\n",
    "#         print(\"tokenized_target_sentence: \", tokenized_target_sentence)\n",
    "        predictions = transformer([tokenized_input_sentence[:,:-1], tokenized_target_sentence])\n",
    "#         print(predictions)\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "#         sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        sampled_token = hin_vectorization.tokenizer.decode([sampled_token_index])\n",
    "#         print(sampled_token)\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print(\"                                        \")\n",
    "\n",
    "# input_sentence = \"I ran\"\n",
    "# translated = decode_sequence(input_sentence)\n",
    "# print(input_sentence)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c829fd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remind me of my pilates class at 6 : 00 pm\n",
      "[start] mujhe 6 00 pm ko meri pilates class ke baare me yaad dilaye [end]\n",
      "                                        \n",
      "Driving time for Phoenix to LA\n",
      "[start] ho raha hu toh mai ho subah ke liye driving time ki zaroorat hai [end]\n",
      "                                        \n",
      "Add 2 minutes to the timer\n",
      "[start] timer me 2 minutes add karen [end]\n",
      "                                        \n",
      "Delete the birthday party reminder .\n",
      "[start] birthday party reminder ko hata do [end]\n",
      "                                        \n",
      "Replay the last song\n",
      "[start] pichla gaana fir se the play kare [end]\n",
      "                                        \n",
      "Remind me to take 5 min break at 4 : 30 pm\n",
      "[start] mujhe 4 30 pm ko 5 minute ka break lene ke liye yaad dilaye [end]\n",
      "                                        \n",
      "set alarm for 8 am\n",
      "[start] subah 8 bajhe ke liye alarm set kare [end]\n",
      "                                        \n",
      "when is traffic lightest driving from here to florida\n",
      "[start] a se a se traffic kab he tak ka l ki hai [end]\n",
      "                                        \n",
      "Iran forecast .\n",
      "[start] ran forecast [end]\n",
      "                                        \n",
      "is it going to freeze over today ?\n",
      "[start] kya aaj ye n freeze hone wala hai [end]\n",
      "                                        \n",
      "Are there any music festivals in Atlanta this summer\n",
      "[start] ya tlan sville me ko me koi play koi music karna event hai hai [end]\n",
      "                                        \n",
      "will it rain in San Diego this week\n",
      "[start] ya is week an i ego me baarish hogi [end]\n",
      "                                        \n",
      "what ' s the weather going to be like in 3 days\n",
      "[start] 3 din me mausam kaisa hone wala hai [end]\n",
      "                                        \n",
      "Cancel alarm for 7 pm\n",
      "[start] 7 pm ke liye alarm ko cancel karen [end]\n",
      "                                        \n",
      "tell me about weather for tomorrow\n",
      "[start] u mujhe aaj mausam ke liye yaad dilao [end]\n",
      "                                        \n",
      "how cold will it be today\n",
      "[start] aj kitni garmi hone wali hai [end]\n",
      "                                        \n",
      "How many miles is a flight from Detroit to London\n",
      "[start] e e e e ki e ke kitne miles ki flight hai [end]\n",
      "                                        \n",
      "Can I have an alarm for Friday July 20\n",
      "[start] ya me friday u ke liye alarm set kijiye for u he [end]\n",
      "                                        \n",
      "Make an alarm go off at noon and 6 pm every day for a month\n",
      "[start] ek mahine ke liye har din dop alarm m banayen har [end]\n",
      "                                        \n",
      "Temperature seattle tonight f\n",
      "[start] aj raat ka temperature f par seattle kijiye [end]\n",
      "                                        \n",
      "Set a reminder for 6 : 30 pm to set up a Facetime chat with Kristen\n",
      "[start] 6 30 pm ka reminder set kare to set up a a a a a a a a a a time set up time set time set up [end]\n",
      "                                        \n",
      "rate this album at the top\n",
      "[start] is album ko top par rate kariye [end]\n",
      "                                        \n",
      "what time is sun rise\n",
      "[start] suraj kis time par niklega [end]\n",
      "                                        \n",
      "play 80s only\n",
      "[start] sirf 80s bajaayen [end]\n",
      "                                        \n",
      "Text Aunt Brenda\n",
      "[start] un logo ke paas mei ke liye text [end]\n",
      "                                        \n",
      "whats the weather radar look like near me\n",
      "[start] mere paas weather radar kaisa dikh raha hai [end]\n",
      "                                        \n",
      "is it shorts weather\n",
      "[start] kya ye shorts ka mausam hai [end]\n",
      "                                        \n",
      "I need directions to get to Newark , DE with the least traffic .\n",
      "[start] mujhe sabse kam traffic me e e e ki e e ki traffic [end]\n",
      "                                        \n",
      "What is the temperature right now\n",
      "[start] abhi ka temperature kya hai [end]\n",
      "                                        \n",
      "create a midday alarm\n",
      "[start] ek midday alarm banaen [end]\n",
      "                                        \n"
     ]
    }
   ],
   "source": [
    "hin_vocab = hin_vectorization.word_index\n",
    "hin_index_lookup = dict(zip(range(1,len(hin_vocab)), hin_vocab))\n",
    "\n",
    "max_decoded_sentence_length = 56\n",
    "\n",
    "hin_index_lookup[0] = \"\"\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = np.array(hin_vectorization([input_sentence]))\n",
    "#     print(\"tokenized_input_sentence: \", tokenized_input_sentence[0])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = hin_vectorization([decoded_sentence])[:, :-1]\n",
    "#         print(\"tokenized_target_sentence: \", tokenized_target_sentence)\n",
    "        predictions = transformer([tokenized_input_sentence[:,:-1], tokenized_target_sentence])\n",
    "#         print(predictions)\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "#         sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        sampled_token = hin_vectorization.tokenizer.decode([sampled_token_index])\n",
    "#         print(sampled_token)\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "# test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(train_eng_texts)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print(\"                                        \")\n",
    "\n",
    "# input_sentence = \"parties in san francisco during the month of december\"\n",
    "# translated = decode_sequence(input_sentence)\n",
    "# print(input_sentence)\n",
    "# print(translated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
